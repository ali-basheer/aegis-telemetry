"""
MODULE: AI_INFERENCE_ENGINE
MODEL: TENSORFLOW_LITE (QUANTIZED)
PROFILE: FORD_SCORPION_V8

DESCRIPTION:
    The runtime execution engine for the Forensic Classifier.
    It loads the trained TFLite model and performs real-time inference 
    on telemetry streams to detect "Defeat Device" signatures.

    Unlike the training script, this module is optimized for low-latency 
    prediction (<5ms) suitable for embedded deployment.
"""

import os
import logging
import numpy as np
import tensorflow as tf

# Configure module-level logger
logger = logging.getLogger("AEGIS.AI")

class AIEngine:
    """
    Wraps the TFLite Interpreter for thread-safe inference.
    """
    def __init__(self, model_path=None):
        if model_path is None:
            # Default location generated by tools/train_dummy_model.py
            base_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            model_path = os.path.join(base_dir, 'data', 'models', 'emissions_v1.tflite')

        self.model_path = model_path
        self.interpreter = None
        self.input_details = None
        self.output_details = None
        
        # Load the Brain
        self._load_model()

    def _load_model(self):
        if not os.path.exists(self.model_path):
            logger.warning(f"AI Model missing at {self.model_path}. Intelligence Layer Disabled.")
            return

        try:
            self.interpreter = tf.lite.Interpreter(model_path=self.model_path)
            self.interpreter.allocate_tensors()
            
            self.input_details = self.interpreter.get_input_details()
            self.output_details = self.interpreter.get_output_details()
            
            logger.info(f"[AI] Neural Network Loaded: {self.model_path}")
            
        except Exception as e:
            logger.error(f"[AI] Failed to load Tensor Core: {e}")
            self.interpreter = None

    def analyze_frame(self, frame: dict) -> dict:
        """
        Runs a single telemetry snapshot through the Neural Network.
        Returns the Verdict and Confidence Score.
        """
        if not self.interpreter:
            return {
                "verdict": "UNAVAILABLE",
                "confidence": 0.0,
                "is_cheating": False
            }

        # 1. PREPROCESS INPUTS
        # Must match the training vector: [RPM, LOAD, TEMP, MAF, REPORTED_NOX]
        # We handle missing keys with defaults to prevent crashes
        input_vector = np.array([[
            frame.get('rpm', 800.0),
            frame.get('load', 0.0),
            frame.get('temp', 40.0),
            frame.get('maf', 10.0),
            frame.get('actual_nox', 0.0)
        ]], dtype=np.float32)

        # 2. RUN INFERENCE
        self.interpreter.set_tensor(self.input_details[0]['index'], input_vector)
        self.interpreter.invoke()
        
        # 3. INTERPRET OUTPUT
        # Model outputs a probability between 0.0 (Clean) and 1.0 (Fraud)
        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])
        cheat_prob = float(output_data[0][0])
        
        # 4. FORMULATE VERDICT
        # We use a high threshold (85%) to avoid false accusations
        is_fraud = cheat_prob > 0.85
        
        verdict = "NON_COMPLIANT" if is_fraud else "COMPLIANT"
        
        return {
            "verdict": verdict,
            "confidence": cheat_prob * 100.0, # Percentage
            "is_cheating": is_fraud,
            "evidence": self._explain_verdict(frame, cheat_prob)
        }

    def _explain_verdict(self, frame, prob):
        """
        Simple Explainable AI (XAI) logic to tell the user WHY it flagged the car.
        """
        reasons = []
        if prob > 0.6:
            if frame.get('load', 0) > 50 and frame.get('actual_nox', 0) < 10:
                reasons.append("HIGH_LOAD_LOW_NOX")
            if frame.get('fuel_rate', 0) > 20 and frame.get('actual_nox', 0) < 5:
                reasons.append("FUEL_DIVERGENCE")
        
        return reasons if reasons else ["MODEL_INFERENCE"]